{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YjKtsX3luGUd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from hyperopt import fmin, tpe, hp, Trials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFoFmKHtsa1U",
        "outputId": "775bb148-5b7e-4ac9-d386-1de663297036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.3.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->lazypredict) (1.16.0)\n",
            "Installing collected packages: lazypredict\n",
            "Successfully installed lazypredict-0.2.12\n"
          ]
        }
      ],
      "source": [
        "!pip install lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7UQ6v15SuUxz"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sRep_DEVuZzs"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(\"X_train.csv\")\n",
        "X_test = pd.read_csv(\"X_test.csv\")\n",
        "y_train = pd.read_csv(\"y_train.csv\")\n",
        "y_test = pd.read_csv(\"y_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOJK0uKWuc-j",
        "outputId": "de35055b-d73f-4d8e-e357-38ce8e35c9af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 31/42 [00:29<00:06,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantileRegressor model failed to execute\n",
            "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 41/42 [00:39<00:01,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4126\n",
            "[LightGBM] [Info] Number of data points in the train set: 2223, number of used features: 69\n",
            "[LightGBM] [Info] Start training from score 3.706928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:39<00:00,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model\n",
            "LGBMRegressor                            0.61\n",
            "HistGradientBoostingRegressor            0.61\n",
            "GradientBoostingRegressor                0.61\n",
            "RandomForestRegressor                    0.62\n",
            "ExtraTreesRegressor                      0.63\n",
            "HuberRegressor                           0.64\n",
            "TransformedTargetRegressor               0.64\n",
            "LinearRegression                         0.64\n",
            "Ridge                                    0.64\n",
            "LassoLarsIC                              0.64\n",
            "RidgeCV                                  0.64\n",
            "LassoCV                                  0.65\n",
            "LassoLarsCV                              0.65\n",
            "LarsCV                                   0.65\n",
            "ElasticNetCV                             0.65\n",
            "BayesianRidge                            0.65\n",
            "XGBRegressor                             0.65\n",
            "LinearSVR                                0.65\n",
            "AdaBoostRegressor                        0.65\n",
            "SVR                                      0.66\n",
            "OrthogonalMatchingPursuit                0.67\n",
            "OrthogonalMatchingPursuitCV              0.67\n",
            "NuSVR                                    0.67\n",
            "BaggingRegressor                         0.67\n",
            "PoissonRegressor                         0.68\n",
            "TweedieRegressor                         0.68\n",
            "Lars                                     0.69\n",
            "GammaRegressor                           0.70\n",
            "KNeighborsRegressor                      0.76\n",
            "MLPRegressor                             0.82\n",
            "ElasticNet                               0.89\n",
            "PassiveAggressiveRegressor               0.90\n",
            "DecisionTreeRegressor                    0.91\n",
            "ExtraTreeRegressor                       0.92\n",
            "DummyRegressor                           1.02\n",
            "Lasso                                    1.02\n",
            "LassoLars                                1.02\n",
            "KernelRidge                              3.80\n",
            "GaussianProcessRegressor                 3.82\n",
            "SGDRegressor                           123.11\n",
            "RANSACRegressor                 3717426866.49\n",
            "Name: RMSE, dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "regressor = LazyRegressor(ignore_warnings=False, custom_metric=None)\n",
        "\n",
        "# Fit and predict using LazyRegressor\n",
        "models, predictions = regressor.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print the models and their performance\n",
        "print(models[\"RMSE\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xunJXHrSpA7z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yn6KNCPak_xY"
      },
      "outputs": [],
      "source": [
        "X_train.columns = [str(i) for i in range(1, len(X_train.columns) + 1)]\n",
        "\n",
        "# Rename columns in X_test with numerical names\n",
        "X_test.columns = [str(i) for i in range(1, len(X_test.columns) + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z1DihWMvK3t",
        "outputId": "f7ddf9bf-8620-456c-9b7d-54851b8b49a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:36<00:00,  2.77trial/s, best loss: 0.5980044944652307]\n",
            "Best Hyperparameters: {'colsample_bytree': 0.5157206647734114, 'learning_rate': 0.05003214030615743, 'max_depth': 14.0, 'num_leaves': 57.0, 'subsample': 0.6297720913065918}\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "# Turn off all warnings\n",
        "# Define the objective function to be minimized\n",
        "def objective(params):\n",
        "    # Convert some hyperparameters to integers\n",
        "    params['num_leaves'] = int(params['num_leaves'])\n",
        "    params['max_depth'] = int(params['max_depth'])\n",
        "\n",
        "    # Define the LightGBM dataset\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "    # Set hyperparameters\n",
        "    params['objective'] = 'regression'\n",
        "    params['metric'] = 'rmse'\n",
        "    params['verbose'] = -1\n",
        "\n",
        "    # Train the LightGBM model\n",
        "    model = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    predictions = model.predict(X_test)\n",
        "    # print(y_test)\n",
        "\n",
        "    # Calculate RMSE (you can replace this with your own evaluation metric)\n",
        "    rmse = np.sqrt(np.mean((predictions - y_test['score']) ** 2))\n",
        "\n",
        "    # Return the value to be minimized (in this case, RMSE)\n",
        "    return rmse\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
        "    'num_leaves': hp.quniform('num_leaves', 20, 100, 1),\n",
        "    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "}\n",
        "\n",
        "# Specify the optimization algorithm (Tree-structured Parzen Estimator)\n",
        "tpe_algorithm = tpe.suggest\n",
        "\n",
        "# Create Trials object to store optimization results\n",
        "trials = Trials()\n",
        "\n",
        "# Run the optimization\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe_algorithm,\n",
        "            trials=trials,\n",
        "            max_evals=100)  # You can adjust the number of evaluations\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMkPsii8mcNv",
        "outputId": "bf7a2db1-cd78-4a71-8690-6c685f3d9059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2223, 146)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY3Y24TKme_F",
        "outputId": "843ed606-b45b-4a4e-f4e3-58477932ce0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(248, 146)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PxktLZmYmgnJ"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zwY6oLvtBet",
        "outputId": "0151eebc-ced8-4265-baec-277a119d5219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:34<00:00,  1.89s/trial, best loss: 0.6006490179879249]\n",
            "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
            "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
            "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
            "Best Hyperparameters for HistGradientBoostingRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
            "Best Hyperparameters for GradientBoostingRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
            "Best Hyperparameters for RandomForestRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
            "Best Hyperparameters for ExtraTreesRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n"
          ]
        }
      ],
      "source": [
        "# Define the objective function to be minimized\n",
        "def objective(params, model):\n",
        "    # Set hyperparameters\n",
        "    params['max_depth'] = int(params['max_depth'])\n",
        "    params['max_iter'] = int(params['max_iter'])\n",
        "    model.set_params(**params)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate RMSE (you can replace this with your own evaluation metric)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "\n",
        "    # Return the value to be minimized (in this case, RMSE)\n",
        "    return rmse\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "}\n",
        "\n",
        "# Specify the optimization algorithm (Tree-structured Parzen Estimator)\n",
        "tpe_algorithm = tpe.suggest\n",
        "\n",
        "# Create Trials object to store optimization results\n",
        "trials = Trials()\n",
        "\n",
        "# Hyperparameter tuning for HistGradientBoostingRegressor\n",
        "hist_params = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
        "    'max_iter': hp.quniform('max_iter', 50, 200, 1),\n",
        "}\n",
        "best_hist = fmin(fn=lambda params: objective(params, HistGradientBoostingRegressor()),\n",
        "                 space=hist_params,\n",
        "                 algo=tpe_algorithm,\n",
        "                 trials=trials,\n",
        "                 max_evals=50)\n",
        "\n",
        "# Hyperparameter tuning for GradientBoostingRegressor\n",
        "gb_params = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
        "}\n",
        "best_gb = fmin(fn=lambda params: objective(params, GradientBoostingRegressor()),\n",
        "               space=gb_params,\n",
        "               algo=tpe_algorithm,\n",
        "               trials=trials,\n",
        "               max_evals=50)\n",
        "\n",
        "# Hyperparameter tuning for RandomForestRegressor\n",
        "rf_params = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
        "}\n",
        "best_rf = fmin(fn=lambda params: objective(params, RandomForestRegressor()),\n",
        "               space=rf_params,\n",
        "               algo=tpe_algorithm,\n",
        "               trials=trials,\n",
        "               max_evals=50)\n",
        "\n",
        "# Hyperparameter tuning for ExtraTreesRegressor\n",
        "et_params = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
        "}\n",
        "best_et = fmin(fn=lambda params: objective(params, ExtraTreesRegressor()),\n",
        "               space=et_params,\n",
        "               algo=tpe_algorithm,\n",
        "               trials=trials,\n",
        "               max_evals=50)\n",
        "\n",
        "# Print the best hyperparameters for each regressor\n",
        "print(\"Best Hyperparameters for HistGradientBoostingRegressor:\", best_hist)\n",
        "print(\"Best Hyperparameters for GradientBoostingRegressor:\", best_gb)\n",
        "print(\"Best Hyperparameters for RandomForestRegressor:\", best_rf)\n",
        "print(\"Best Hyperparameters for ExtraTreesRegressor:\", best_et)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPHbZU_ttJxs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
