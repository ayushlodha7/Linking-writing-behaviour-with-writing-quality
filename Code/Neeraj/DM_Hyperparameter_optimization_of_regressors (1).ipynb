{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YjKtsX3luGUd"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/ayushlodha/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <3AF1EF0C-311C-31EC-BCE3-679F37ABEE16> /Users/ayushlodha/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/ayushlodha/Desktop/Sem3-MCDS/CSCI57300-DataMining/Project/linking-writing-processes-to-writing-quality-local/Code/Neeraj/DM_Hyperparameter_optimization_of_regressors (1).ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushlodha/Desktop/Sem3-MCDS/CSCI57300-DataMining/Project/linking-writing-processes-to-writing-quality-local/Code/Neeraj/DM_Hyperparameter_optimization_of_regressors%20%281%29.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ayushlodha/Desktop/Sem3-MCDS/CSCI57300-DataMining/Project/linking-writing-processes-to-writing-quality-local/Code/Neeraj/DM_Hyperparameter_optimization_of_regressors%20%281%29.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushlodha/Desktop/Sem3-MCDS/CSCI57300-DataMining/Project/linking-writing-processes-to-writing-quality-local/Code/Neeraj/DM_Hyperparameter_optimization_of_regressors%20%281%29.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushlodha/Desktop/Sem3-MCDS/CSCI57300-DataMining/Project/linking-writing-processes-to-writing-quality-local/Code/Neeraj/DM_Hyperparameter_optimization_of_regressors%20%281%29.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhyperopt\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin, tpe, hp, Trials\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/__init__.py:8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbasic\u001b[39;00m \u001b[39mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/basic.py:226\u001b[0m\n\u001b[1;32m    224\u001b[0m     _LIB \u001b[39m=\u001b[39m Mock(ctypes\u001b[39m.\u001b[39mCDLL)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     _LIB \u001b[39m=\u001b[39m _load_lib()\n\u001b[1;32m    229\u001b[0m _NUMERIC_TYPES \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m)\n\u001b[1;32m    230\u001b[0m _ArrayLike \u001b[39m=\u001b[39m Union[List, np\u001b[39m.\u001b[39mndarray, pd_Series]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/basic.py:211\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load LightGBM library.\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m lib_path \u001b[39m=\u001b[39m find_lib_path()\n\u001b[0;32m--> 211\u001b[0m lib \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39;49mcdll\u001b[39m.\u001b[39;49mLoadLibrary(lib_path[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    212\u001b[0m lib\u001b[39m.\u001b[39mLGBM_GetLastError\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_char_p\n\u001b[1;32m    213\u001b[0m callback \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mCFUNCTYPE(\u001b[39mNone\u001b[39;00m, ctypes\u001b[39m.\u001b[39mc_char_p)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lazypredict_env/lib/python3.8/ctypes/__init__.py:451\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadLibrary\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dlltype(name)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lazypredict_env/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/ayushlodha/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <3AF1EF0C-311C-31EC-BCE3-679F37ABEE16> /Users/ayushlodha/opt/anaconda3/envs/lazypredict_env/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFoFmKHtsa1U",
    "outputId": "775bb148-5b7e-4ac9-d386-1de663297036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.3.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.1.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.11.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2023.3.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->lazypredict) (1.16.0)\n",
      "Installing collected packages: lazypredict\n",
      "Successfully installed lazypredict-0.2.12\n"
     ]
    }
   ],
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UQ6v15SuUxz"
   },
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRep_DEVuZzs"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOJK0uKWuc-j",
    "outputId": "de35055b-d73f-4d8e-e357-38ce8e35c9af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 31/42 [00:29<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileRegressor model failed to execute\n",
      "Solver interior-point is not anymore available in SciPy >= 1.11.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 41/42 [00:39<00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4126\n",
      "[LightGBM] [Info] Number of data points in the train set: 2223, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 3.706928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:39<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "LGBMRegressor                            0.61\n",
      "HistGradientBoostingRegressor            0.61\n",
      "GradientBoostingRegressor                0.61\n",
      "RandomForestRegressor                    0.62\n",
      "ExtraTreesRegressor                      0.63\n",
      "HuberRegressor                           0.64\n",
      "TransformedTargetRegressor               0.64\n",
      "LinearRegression                         0.64\n",
      "Ridge                                    0.64\n",
      "LassoLarsIC                              0.64\n",
      "RidgeCV                                  0.64\n",
      "LassoCV                                  0.65\n",
      "LassoLarsCV                              0.65\n",
      "LarsCV                                   0.65\n",
      "ElasticNetCV                             0.65\n",
      "BayesianRidge                            0.65\n",
      "XGBRegressor                             0.65\n",
      "LinearSVR                                0.65\n",
      "AdaBoostRegressor                        0.65\n",
      "SVR                                      0.66\n",
      "OrthogonalMatchingPursuit                0.67\n",
      "OrthogonalMatchingPursuitCV              0.67\n",
      "NuSVR                                    0.67\n",
      "BaggingRegressor                         0.67\n",
      "PoissonRegressor                         0.68\n",
      "TweedieRegressor                         0.68\n",
      "Lars                                     0.69\n",
      "GammaRegressor                           0.70\n",
      "KNeighborsRegressor                      0.76\n",
      "MLPRegressor                             0.82\n",
      "ElasticNet                               0.89\n",
      "PassiveAggressiveRegressor               0.90\n",
      "DecisionTreeRegressor                    0.91\n",
      "ExtraTreeRegressor                       0.92\n",
      "DummyRegressor                           1.02\n",
      "Lasso                                    1.02\n",
      "LassoLars                                1.02\n",
      "KernelRidge                              3.80\n",
      "GaussianProcessRegressor                 3.82\n",
      "SGDRegressor                           123.11\n",
      "RANSACRegressor                 3717426866.49\n",
      "Name: RMSE, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = LazyRegressor(ignore_warnings=False, custom_metric=None)\n",
    "\n",
    "# Fit and predict using LazyRegressor\n",
    "models, predictions = regressor.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print the models and their performance\n",
    "print(models[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xunJXHrSpA7z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn6KNCPak_xY"
   },
   "outputs": [],
   "source": [
    "X_train.columns = [str(i) for i in range(1, len(X_train.columns) + 1)]\n",
    "\n",
    "# Rename columns in X_test with numerical names\n",
    "X_test.columns = [str(i) for i in range(1, len(X_test.columns) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Z1DihWMvK3t",
    "outputId": "f7ddf9bf-8620-456c-9b7d-54851b8b49a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.77trial/s, best loss: 0.5980044944652307]\n",
      "Best Hyperparameters: {'colsample_bytree': 0.5157206647734114, 'learning_rate': 0.05003214030615743, 'max_depth': 14.0, 'num_leaves': 57.0, 'subsample': 0.6297720913065918}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Turn off all warnings\n",
    "# Define the objective function to be minimized\n",
    "def objective(params):\n",
    "    # Convert some hyperparameters to integers\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    # Define the LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Set hyperparameters\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = model.predict(X_test)\n",
    "    # print(y_test)\n",
    "\n",
    "    # Calculate RMSE (you can replace this with your own evaluation metric)\n",
    "    rmse = np.sqrt(np.mean((predictions - y_test['score']) ** 2))\n",
    "\n",
    "    # Return the value to be minimized (in this case, RMSE)\n",
    "    return rmse\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 100, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "}\n",
    "\n",
    "# Specify the optimization algorithm (Tree-structured Parzen Estimator)\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Create Trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe_algorithm,\n",
    "            trials=trials,\n",
    "            max_evals=100)  # You can adjust the number of evaluations\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMkPsii8mcNv",
    "outputId": "bf7a2db1-cd78-4a71-8690-6c685f3d9059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2223, 146)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AY3Y24TKme_F",
    "outputId": "843ed606-b45b-4a4e-f4e3-58477932ce0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 146)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxktLZmYmgnJ"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zwY6oLvtBet",
    "outputId": "0151eebc-ced8-4265-baec-277a119d5219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:34<00:00,  1.89s/trial, best loss: 0.6006490179879249]\n",
      "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
      "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
      "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
      "Best Hyperparameters for HistGradientBoostingRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
      "Best Hyperparameters for GradientBoostingRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
      "Best Hyperparameters for RandomForestRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n",
      "Best Hyperparameters for ExtraTreesRegressor: {'learning_rate': 0.0959216871441787, 'max_depth': 5.0, 'max_iter': 171.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function to be minimized\n",
    "def objective(params, model):\n",
    "    # Set hyperparameters\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['max_iter'] = int(params['max_iter'])\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE (you can replace this with your own evaluation metric)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "    # Return the value to be minimized (in this case, RMSE)\n",
    "    return rmse\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "}\n",
    "\n",
    "# Specify the optimization algorithm (Tree-structured Parzen Estimator)\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Create Trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Hyperparameter tuning for HistGradientBoostingRegressor\n",
    "hist_params = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
    "    'max_iter': hp.quniform('max_iter', 50, 200, 1),\n",
    "}\n",
    "best_hist = fmin(fn=lambda params: objective(params, HistGradientBoostingRegressor()),\n",
    "                 space=hist_params,\n",
    "                 algo=tpe_algorithm,\n",
    "                 trials=trials,\n",
    "                 max_evals=50)\n",
    "\n",
    "# Hyperparameter tuning for GradientBoostingRegressor\n",
    "gb_params = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
    "}\n",
    "best_gb = fmin(fn=lambda params: objective(params, GradientBoostingRegressor()),\n",
    "               space=gb_params,\n",
    "               algo=tpe_algorithm,\n",
    "               trials=trials,\n",
    "               max_evals=50)\n",
    "\n",
    "# Hyperparameter tuning for RandomForestRegressor\n",
    "rf_params = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
    "}\n",
    "best_rf = fmin(fn=lambda params: objective(params, RandomForestRegressor()),\n",
    "               space=rf_params,\n",
    "               algo=tpe_algorithm,\n",
    "               trials=trials,\n",
    "               max_evals=50)\n",
    "\n",
    "# Hyperparameter tuning for ExtraTreesRegressor\n",
    "et_params = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 20, 1),\n",
    "}\n",
    "best_et = fmin(fn=lambda params: objective(params, ExtraTreesRegressor()),\n",
    "               space=et_params,\n",
    "               algo=tpe_algorithm,\n",
    "               trials=trials,\n",
    "               max_evals=50)\n",
    "\n",
    "# Print the best hyperparameters for each regressor\n",
    "print(\"Best Hyperparameters for HistGradientBoostingRegressor:\", best_hist)\n",
    "print(\"Best Hyperparameters for GradientBoostingRegressor:\", best_gb)\n",
    "print(\"Best Hyperparameters for RandomForestRegressor:\", best_rf)\n",
    "print(\"Best Hyperparameters for ExtraTreesRegressor:\", best_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPHbZU_ttJxs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
